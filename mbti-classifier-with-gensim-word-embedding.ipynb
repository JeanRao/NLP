{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\65873\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\65873\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\65873\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "%matplotlib inline\n",
    "from nltk import tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras import initializers as initializers, regularizers, constraints\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('mbti_1.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the preprocessing include lemmatization and removes stop words, do not do stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer() \n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def cleaning_data(data, remove_stop_words=True):\n",
    "    list_posts = []\n",
    "    i=0   \n",
    "    for row in data.iterrows():\n",
    "        posts = row[1].posts\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', posts) #remove urls\n",
    "        temp = re.sub(\"[^a-zA-Z.]\", \" \", temp) #remove all punctuations except fullstops.\n",
    "        temp = re.sub(' +', ' ', temp).lower() \n",
    "        temp=re.sub(r'\\.+', \".\", temp) #remove multiple fullstops.\n",
    "        if remove_stop_words:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n",
    "        else:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "        list_posts.append(temp)\n",
    "\n",
    "    text = np.array(list_posts)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfp intj moment sportscenter top ten play pr...</td>\n",
       "      <td>INFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>finding lack post alarming. sex boring positi...</td>\n",
       "      <td>ENTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good one course say know blessing curse. abso...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear intp enjoyed conversation day. esoteric ...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fired. another silly misconception. approachi...</td>\n",
       "      <td>ENTJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  type\n",
       "0   enfp intj moment sportscenter top ten play pr...  INFJ\n",
       "1   finding lack post alarming. sex boring positi...  ENTP\n",
       "2   good one course say know blessing curse. abso...  INTP\n",
       "3   dear intp enjoyed conversation day. esoteric ...  INTJ\n",
       "4   fired. another silly misconception. approachi...  ENTJ"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = cleaning_data(data, remove_stop_words=True)\n",
    "data['clean_text']=clean_text\n",
    "data = data[['clean_text', 'type']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total types: 16\n",
      "type\n",
      "ENFJ     190\n",
      "ENFP     675\n",
      "ENTJ     231\n",
      "ENTP     685\n",
      "ESFJ      42\n",
      "ESFP      48\n",
      "ESTJ      39\n",
      "ESTP      89\n",
      "INFJ    1470\n",
      "INFP    1832\n",
      "INTJ    1091\n",
      "INTP    1304\n",
      "ISFJ     166\n",
      "ISFP     271\n",
      "ISTJ     205\n",
      "ISTP     337\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "types=data['type']\n",
    "text=data['clean_text']\n",
    "tps=data.groupby('type')\n",
    "print(\"total types:\",tps.ngroups)\n",
    "print(tps.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in data['clean_text']:\n",
    "    post=post.strip(' ').split('.')\n",
    "    for s in post:\n",
    "        if s.strip(' ').split(' ')!=['']:\n",
    "            sentences.append(s.strip(' ').split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 250  # Word vector dimensionality\n",
    "min_word_count = 1 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 7      # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    }
   ],
   "source": [
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          vector_size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-062c889a6fcb>:2: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"100features_10context\"\n",
    "model_path = F\"C:/Users/65873/Downloads/NLP project/classification{model_name}\"\n",
    "model.save(model_path)\n",
    "print(\"model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comedy', 0.8270348906517029),\n",
       " ('movies', 0.8157392144203186),\n",
       " ('disney', 0.8114413022994995),\n",
       " ('film', 0.8023772835731506),\n",
       " ('animated', 0.8007003664970398),\n",
       " ('thriller', 0.7653870582580566),\n",
       " ('pixar', 0.7639893293380737),\n",
       " ('films', 0.7565824389457703),\n",
       " ('cartoon', 0.7564561367034912),\n",
       " ('television', 0.7562302947044373)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "model_name = \"100features_10context\"\n",
    "model_path = F\"C:/Users/65873/Downloads/NLP project/classification{model_name}\"\n",
    "embed_model = word2vec.Word2Vec.load(model_path)\n",
    "embed_model.wv.most_similar('horror')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=[]\n",
    "for post in data['clean_text']:\n",
    "    p=[]\n",
    "    post=post.strip(' ').split('.')\n",
    "    for s in post:\n",
    "        if s.strip(' ').split(' ')!=['']:\n",
    "            p+=s.strip(' ').split(' ')\n",
    "    sequences.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model.wv['shake']\n",
    "padvec=np.zeros(250)\n",
    "input_sequences=[]\n",
    "for i in range(len(sequences)):\n",
    "    post=[]\n",
    "    for j in range(len(sequences[i])):\n",
    "        if sequences[i][j] in embed_model.wv:\n",
    "            post.append(embed_model.wv[sequences[i][j]])\n",
    "    if len(post)<300:\n",
    "        for k in range(300-len(post)):\n",
    "            post.append(padvec)\n",
    "    post=np.array(post[:300])\n",
    "    input_sequences.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8675"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh=[[i] for i in data['type']]\n",
    "# define one hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "# transform data\n",
    "onehot = encoder.fit_transform(oh)\n",
    "print(onehot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "typecode = {\"type\":{\"INFJ\": 1, \"INFP\": 2, \"INTJ\":3, \"INTP\":4, \"ISFJ\":5, \"ISFP\":6, \n",
    "                   \"ISTJ\":7, \"ISTP\":8, \"ENFJ\":9, \"ENFP\":10, \"ENTJ\":11, \"ENTP\":12, \"ESFJ\":13, \"ESFP\":14, \"ESTJ\":15, \"ESTP\":16}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(typecode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1       12\n",
       "2        4\n",
       "3        3\n",
       "4       11\n",
       "        ..\n",
       "8670     6\n",
       "8671    10\n",
       "8672     4\n",
       "8673     2\n",
       "8674     2\n",
       "Name: type, Length: 8675, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys=np.array(onehot)\n",
    "xs=np.array(input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=1, stratify=ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model and train, oveall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "217/217 [==============================] - 63s 271ms/step - loss: 2.3535 - acc: 0.1948 - val_loss: 2.2973 - val_acc: 0.1695\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 55s 255ms/step - loss: 2.2758 - acc: 0.2083 - val_loss: 2.2811 - val_acc: 0.2104\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 62s 285ms/step - loss: 2.2854 - acc: 0.2095 - val_loss: 2.2842 - val_acc: 0.2115\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(tf.keras.Input(shape=(300,250)))\n",
    "model.add(tf.keras.layers.Conv1D(32, (30), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Conv1D(32, (20), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size =4 , strides = 2))\n",
    "model.add(tf.keras.layers.Dropout(0.3)) \n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(64, (10), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Conv1D(64, (5), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size =4 , strides = 2))\n",
    "model.add(tf.keras.layers.Dropout(0.3)) \n",
    "\n",
    "model.add(LSTM(64,return_sequences=True))\n",
    "model.add(tf.keras.layers.Dropout(0.2)) \n",
    "model.add(LSTM(128))\n",
    "#model.add(Bidirectional(LSTM(64)))\n",
    "\n",
    "model.add(Dense(16, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
    "history = model.fit(x_train, y_train, epochs = 3, validation_data=(x_test, y_test), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 4s 75ms/step - loss: 2.2842 - acc: 0.2115\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
